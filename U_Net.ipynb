{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP0jhC+XBeXLpLK5MHk7dDv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LoPA607/SOC-2024/blob/main/U_Net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6o24xZb15wXf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#for double convlutional layer\n",
        "def double_conv(in_c, out_c):\n",
        "  conv=nn.Sequential(\n",
        "      nn.Conv2d(in_c, out_c, kernel_size=3),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.Conv2d(out_c, out_c, kernel_size=3),\n",
        "      nn.ReLU(inplace=True),\n",
        "  )\n",
        "  return conv"
      ],
      "metadata": {
        "id": "pA81OxmU53jI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crop_img(tensor, target_tensor):\n",
        "  target_size=target_tensor.size()[2]\n",
        "  tensor_size=tensor.size()[2]\n",
        "  delta=tensor_size-target_size\n",
        "  delta=delta // 2\n",
        "  return tensor[:, :, delta:tensor_size-delta, delta:tensor_size-delta]"
      ],
      "metadata": {
        "id": "z8Bq8f5YDcJs"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(UNet, self).__init__()\n",
        "    self.max_pool_2x2=nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    self.down_conv_1=double_conv(1,64)   #input=1channel and output=64channels\n",
        "    self.down_conv_2=double_conv(64,128)\n",
        "    self.down_conv_3=double_conv(128,256)\n",
        "    self.down_conv_4=double_conv(256,512)\n",
        "    self.down_conv_5=double_conv(512,1024)\n",
        "\n",
        "    self.up_trans_1=nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=2, stride=2)\n",
        "    self.up_conv_1=double_conv(1024, 512)\n",
        "\n",
        "    self.up_trans_2=nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=2, stride=2)\n",
        "    self.up_conv_2=double_conv(512, 256)\n",
        "\n",
        "    self.up_trans_3=nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=2, stride=2)\n",
        "    self.up_conv_3=double_conv(256, 128)\n",
        "\n",
        "    self.up_trans_4=nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=2, stride=2)\n",
        "    self.up_conv_4=double_conv(128, 64)\n",
        "\n",
        "    self.out=nn.Conv2d(in_channels=64, out_channels=2, kernel_size=1)\n",
        "\n",
        "\n",
        "  def forward(self, image):\n",
        "    #encoder\n",
        "    x1=self.down_conv_1(image) #\n",
        "    print(x1.size())\n",
        "    x2=self.max_pool_2x2(x1)\n",
        "    x3=self.down_conv_2(x2) #\n",
        "    x4=self.max_pool_2x2(x3)\n",
        "    x5=self.down_conv_3(x4) #\n",
        "    x6=self.max_pool_2x2(x5)\n",
        "    x7=self.down_conv_4(x6) #\n",
        "    x8=self.max_pool_2x2(x7)\n",
        "    x9=self.down_conv_5(x8)\n",
        "\n",
        "    #decoder\n",
        "    x = self.up_trans_1(x9)\n",
        "    y = crop_img(x7,x)\n",
        "    x=self.up_conv_1(torch.cat([x, y], 1))\n",
        "\n",
        "    x = self.up_trans_2(x)\n",
        "    y = crop_img(x5,x)\n",
        "    x=self.up_conv_2(torch.cat([x, y], 1))\n",
        "\n",
        "    x = self.up_trans_3(x)\n",
        "    y = crop_img(x3,x)\n",
        "    x=self.up_conv_3(torch.cat([x, y], 1))\n",
        "\n",
        "    x = self.up_trans_4(x)\n",
        "    y = crop_img(x1,x)\n",
        "    x=self.up_conv_4(torch.cat([x, y], 1))\n",
        "\n",
        "    x=self.out(x)\n",
        "    print(x.size())\n",
        "    return x\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hnlN-P7l6CuK"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  image = torch.rand((1, 1, 572, 572))\n",
        "  model = UNet()\n",
        "  print(model(image))"
      ],
      "metadata": {
        "id": "cwdS5yPB-zQn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90740e9e-2be3-4744-8841-9fa426034a22"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 64, 568, 568])\n",
            "torch.Size([1, 2, 388, 388])\n",
            "tensor([[[[-1.1598e-03,  1.2682e-03,  1.3640e-03,  ...,  1.1839e-03,\n",
            "            3.1728e-03,  4.0019e-03],\n",
            "          [-1.1055e-03,  4.2946e-03,  3.3119e-03,  ...,  9.9182e-05,\n",
            "           -1.1561e-03,  2.8131e-03],\n",
            "          [ 2.0560e-03, -7.4171e-06,  1.3258e-03,  ..., -1.5969e-03,\n",
            "            1.2280e-03,  2.0878e-03],\n",
            "          ...,\n",
            "          [ 5.0114e-05, -1.4733e-03,  3.8401e-04,  ...,  6.2406e-03,\n",
            "            3.9680e-03,  3.1526e-04],\n",
            "          [ 5.4194e-03,  4.8651e-03,  2.3115e-03,  ...,  2.9118e-03,\n",
            "            3.0722e-03,  2.4935e-03],\n",
            "          [-1.9208e-03, -2.1064e-03,  8.4147e-05,  ..., -8.1116e-05,\n",
            "            7.7536e-03,  3.4603e-03]],\n",
            "\n",
            "         [[ 8.4500e-02,  8.4862e-02,  9.0486e-02,  ...,  8.7925e-02,\n",
            "            8.8599e-02,  9.2674e-02],\n",
            "          [ 9.0252e-02,  8.4636e-02,  8.3281e-02,  ...,  8.7208e-02,\n",
            "            8.1785e-02,  8.6667e-02],\n",
            "          [ 8.1650e-02,  9.1257e-02,  8.5416e-02,  ...,  8.3140e-02,\n",
            "            8.7528e-02,  8.5419e-02],\n",
            "          ...,\n",
            "          [ 8.0194e-02,  8.8920e-02,  8.0846e-02,  ...,  7.9206e-02,\n",
            "            8.9014e-02,  8.7727e-02],\n",
            "          [ 8.5105e-02,  8.0118e-02,  8.7099e-02,  ...,  8.8519e-02,\n",
            "            8.1396e-02,  8.7036e-02],\n",
            "          [ 8.9714e-02,  9.4300e-02,  8.7022e-02,  ...,  8.1073e-02,\n",
            "            8.0827e-02,  8.4264e-02]]]], grad_fn=<ConvolutionBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Iw77nkrMAXc_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}