{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LoPA607/SOC-2024/blob/main/Week5%20%2B%20Week6/latent_diffusion_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-DzPTovKVPP"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import imageio\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import Compose,ToTensor,Lambda\n",
        "from torchvision.datasets.mnist import MNIST\n",
        "from tqdm.auto import tqdm\n",
        "from argparse import ArgumentParser\n",
        "import yaml\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWLG9s3VLsnt"
      },
      "outputs": [],
      "source": [
        "device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdEchPqNMP89"
      },
      "source": [
        "## implement LPIPS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Yw7rkLoL9XB"
      },
      "outputs": [],
      "source": [
        "def spatial_average(in_tens, keepdim=True):\n",
        "  return in_tens.mean([2,3], keepdim=keepdim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMeqhQJlMLoS"
      },
      "outputs": [],
      "source": [
        "class vgg16(torch.nn.Module):\n",
        "  def __init__(self, requires_grad=False, pretrained=True):\n",
        "    vgg_pretrained_features=torchvision.models.vgg16(pretrained=pretrained).vgg_pretrained_features\n",
        "    self.slice1=torch.nn.Sequential()\n",
        "    self.slice2=torch.nn.Sequential()\n",
        "    self.slice3=torch.nn.Sequential()\n",
        "    self.slice4=torch.nn.Sequential()\n",
        "    self.slice5=torch.nn.Sequential()\n",
        "    self.N_slices=5\n",
        "    for x in range(4):\n",
        "      self.slice1.add_module(str(x), vgg_pretrained_features[x])\n",
        "    for x in range(4,9):\n",
        "      self.slice2.add_module(str(x), vgg_pretrained_features[x])\n",
        "    for x in range(9,16):\n",
        "      self.slice3.add_module(str(x), vgg_pretrained_features[x])\n",
        "    for x in range(16,23):\n",
        "      self.slice4.add_module(str(x), vgg_pretrained_features[x])\n",
        "    for x in range(23,30):\n",
        "      self.slice5.add_module(str(x), vgg_pretrained_features[x])\n",
        "    #freeze vgg model\n",
        "    if not requires_grad:\n",
        "      for param in self.parameters():\n",
        "        param.requires_grad=False\n",
        "\n",
        "\n",
        "    def forward(self, X):\n",
        "        h=self.slice1(X)\n",
        "        h_relu1_2=h\n",
        "        h=self.slice2(h)\n",
        "        h_relu2_2=h\n",
        "        h=self.slice3(h)\n",
        "        h_relu3_3=h\n",
        "        h=self.slice4(h)\n",
        "        h_relu4_3=h\n",
        "        h=self.slice5(h)\n",
        "        h_relu5_3=h\n",
        "        vgg_output=nametuple(\"VggOutputs\", ['relu1_2', 'relu2_2', 'relu3_3', 'relu4_3', 'relu5_3'])\n",
        "        out=vgg_output(h_relu1_2,h_relu2_2,h_relu3_3,h_relu4_3,h_relu5_3)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QPpNb4yTaVf"
      },
      "outputs": [],
      "source": [
        "# to get imagenet normalisation\n",
        "class ScalingLayer(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ScalingLayer, self). __init__()\n",
        "    self.register_buffer('shift', torch.Tensor([-.030,-.088,-.188])[None,:,None,None])\n",
        "    self.register_buffer('scale', torch.Tensor([.458, .448, .450])[None,:,None,None])\n",
        "\n",
        "  def forward(self, inp):\n",
        "    return (inp - self.shift) / self.scale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "caMjjvsmVy14"
      },
      "outputs": [],
      "source": [
        "class NetLinLayer(nn.Module):\n",
        "  def __init__(self, chn_in,chn_out=1, use_dropout=False):\n",
        "    super(NetLinLayer, self).__init__()\n",
        "    layers=[nn.Dropout(), ] if (use_dropout) else []\n",
        "    layers+=[nn.Conv2d(chn_in,chn_out,kernel_size=1,stride=1, padding=0,bias=False), ]\n",
        "    self.model=nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out=self.model(x)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoF8-FE-UoKa"
      },
      "outputs": [],
      "source": [
        "class LPIPS(nn.Module):\n",
        "    def __init__(self, net='vgg', version='0.1', use_dropout=True):\n",
        "        super(LPIPS, self).__init__()\n",
        "        self.version = version\n",
        "        self.sclaing_layer=SclaingLayer()\n",
        "        self.chns=[64, 128, 256, 512, 512]\n",
        "        self.L=len(self.chns)\n",
        "        self.net=vgg16(requires_grad=False, pretrained=True)\n",
        "\n",
        "        self.lin0=NetLinLayer(self.chns[0], use_dropout=use_dropout)\n",
        "        self.lin1=NetLinLayer(self.chns[1], use_dropout=use_dropout)\n",
        "        self.lin2=NetLinLayer(self.chns[2], use_dropout=use_dropout)\n",
        "        self.lin3=NetLinLayer(self.chns[3], use_dropout=use_dropout)\n",
        "        self.lin4=NetLinLayer(self.chns[4], use_dropout=use_dropout)\n",
        "        self.lins=[self.lin0, self.lin1, self.lin2, self.lin3, self.lin4]\n",
        "        self.lins=nn.ModuleList(self.lins)\n",
        "\n",
        "        import inspect\n",
        "        import os\n",
        "\n",
        "        model_path=os.path.abspath(\n",
        "            os.path.join(inspect.getfile(self. __init__),'..','weights/v%s/%s.pth'%(version, net))\n",
        "        )\n",
        "        print('Loading model from: %s' % model_path)\n",
        "        self.load_state_dict(torch.load(model_path, map_loaction=device), strict=False)\n",
        "\n",
        "\n",
        "        self.eval()\n",
        "        for param in self.parameters():\n",
        "          param.requires_grad=False\n",
        "\n",
        "    def forward(self, in0,in1, normalize=False):\n",
        "      if normalize:\n",
        "        in0=2*in0-1\n",
        "        in1=2*in1-1\n",
        "\n",
        "      in0_input, in1_input=self.scaling_layer(in0),self.scaling_layer(in1)\n",
        "      outs0,outs1=self.net.forward(in0_input),self.net.forward(in1_input)\n",
        "      feats0,feats1,diffs={},{},{}\n",
        "\n",
        "      for i in range(self.L):\n",
        "        feats0[i]=torch.nn.functional.normalize((outs0[i]), dim=1)\n",
        "        feats1[i]=torch.nn.functional.normalize(outs1[i])\n",
        "        diffs[i]=(feats0[i]-feats1[i])**2\n",
        "\n",
        "      res=[spatial_average(self.lins[i](diffs[i]),keepdim=True) for i in range(self.L)]\n",
        "      val=0\n",
        "\n",
        "      for l in range(self.L):\n",
        "        val+=res[l]\n",
        "\n",
        "      return val\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOHIEXu00Q56"
      },
      "source": [
        "##implement autoencoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzHaXytF0QOC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DownBlock(nn.Module):\n",
        "    r\"\"\"\n",
        "    Down conv block with attention.\n",
        "    Sequence of following block\n",
        "    1. Resnet block with time embedding\n",
        "    2. Attention block\n",
        "    3. Downsample\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, t_emb_dim,\n",
        "                 down_sample, num_heads, num_layers, attn, norm_channels, cross_attn=False, context_dim=None):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.down_sample = down_sample\n",
        "        self.attn = attn\n",
        "        self.context_dim = context_dim\n",
        "        self.cross_attn = cross_attn\n",
        "        self.t_emb_dim = t_emb_dim\n",
        "        self.resnet_conv_first = nn.ModuleList(\n",
        "            [\n",
        "                nn.Sequential(\n",
        "                    nn.GroupNorm(norm_channels, in_channels if i == 0 else out_channels),\n",
        "                    nn.SiLU(),\n",
        "                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels,\n",
        "                              kernel_size=3, stride=1, padding=1),\n",
        "                )\n",
        "                for i in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "        if self.t_emb_dim is not None:\n",
        "            self.t_emb_layers = nn.ModuleList([\n",
        "                nn.Sequential(\n",
        "                    nn.SiLU(),\n",
        "                    nn.Linear(self.t_emb_dim, out_channels)\n",
        "                )\n",
        "                for _ in range(num_layers)\n",
        "            ])\n",
        "        self.resnet_conv_second = nn.ModuleList(\n",
        "            [\n",
        "                nn.Sequential(\n",
        "                    nn.GroupNorm(norm_channels, out_channels),\n",
        "                    nn.SiLU(),\n",
        "                    nn.Conv2d(out_channels, out_channels,\n",
        "                              kernel_size=3, stride=1, padding=1),\n",
        "                )\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        if self.attn:\n",
        "            self.attention_norms = nn.ModuleList(\n",
        "                [nn.GroupNorm(norm_channels, out_channels)\n",
        "                 for _ in range(num_layers)]\n",
        "            )\n",
        "\n",
        "            self.attentions = nn.ModuleList(\n",
        "                [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
        "                 for _ in range(num_layers)]\n",
        "            )\n",
        "\n",
        "        if self.cross_attn:\n",
        "            assert context_dim is not None, \"Context Dimension must be passed for cross attention\"\n",
        "            self.cross_attention_norms = nn.ModuleList(\n",
        "                [nn.GroupNorm(norm_channels, out_channels)\n",
        "                 for _ in range(num_layers)]\n",
        "            )\n",
        "            self.cross_attentions = nn.ModuleList(\n",
        "                [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
        "                 for _ in range(num_layers)]\n",
        "            )\n",
        "            self.context_proj = nn.ModuleList(\n",
        "                [nn.Linear(context_dim, out_channels)\n",
        "                 for _ in range(num_layers)]\n",
        "            )\n",
        "\n",
        "        self.residual_input_conv = nn.ModuleList(\n",
        "            [\n",
        "                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n",
        "                for i in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.down_sample_conv = nn.Conv2d(out_channels, out_channels,\n",
        "                                          4, 2, 1) if self.down_sample else nn.Identity()\n",
        "\n",
        "    def forward(self, x, t_emb=None, context=None):\n",
        "        out = x\n",
        "        for i in range(self.num_layers):\n",
        "            # Resnet block of Unet\n",
        "            resnet_input = out\n",
        "            out = self.resnet_conv_first[i](out)\n",
        "            if self.t_emb_dim is not None:\n",
        "                out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n",
        "            out = self.resnet_conv_second[i](out)\n",
        "            out = out + self.residual_input_conv[i](resnet_input)\n",
        "\n",
        "            if self.attn:\n",
        "                # Attention block of Unet\n",
        "                batch_size, channels, h, w = out.shape\n",
        "                in_attn = out.reshape(batch_size, channels, h * w)\n",
        "                in_attn = self.attention_norms[i](in_attn)\n",
        "                in_attn = in_attn.transpose(1, 2)\n",
        "                out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
        "                out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
        "                out = out + out_attn\n",
        "\n",
        "            if self.cross_attn:\n",
        "                assert context is not None, \"context cannot be None if cross attention layers are used\"\n",
        "                batch_size, channels, h, w = out.shape\n",
        "                in_attn = out.reshape(batch_size, channels, h * w)\n",
        "                in_attn = self.cross_attention_norms[i](in_attn)\n",
        "                in_attn = in_attn.transpose(1, 2)\n",
        "                assert context.shape[0] == x.shape[0] and context.shape[-1] == self.context_dim\n",
        "                context_proj = self.context_proj[i](context)\n",
        "                out_attn, _ = self.cross_attentions[i](in_attn, context_proj, context_proj)\n",
        "                out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
        "                out = out + out_attn\n",
        "\n",
        "        # Downsample\n",
        "        out = self.down_sample_conv(out)\n",
        "        return out\n",
        "\n"
      ],
      "metadata": {
        "id": "E_qMKjiE_0o2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MidBlock(nn.Module):\n",
        "    r\"\"\"\n",
        "    Mid conv block with attention.\n",
        "    Sequence of following blocks\n",
        "    1. Resnet block with time embedding\n",
        "    2. Attention block\n",
        "    3. Resnet block with time embedding\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, t_emb_dim, num_heads, num_layers, norm_channels, cross_attn=None, context_dim=None):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.t_emb_dim = t_emb_dim\n",
        "        self.context_dim = context_dim\n",
        "        self.cross_attn = cross_attn\n",
        "        self.resnet_conv_first = nn.ModuleList(\n",
        "            [\n",
        "                nn.Sequential(\n",
        "                    nn.GroupNorm(norm_channels, in_channels if i == 0 else out_channels),\n",
        "                    nn.SiLU(),\n",
        "                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, stride=1,\n",
        "                              padding=1),\n",
        "                )\n",
        "                for i in range(num_layers + 1)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        if self.t_emb_dim is not None:\n",
        "            self.t_emb_layers = nn.ModuleList([\n",
        "                nn.Sequential(\n",
        "                    nn.SiLU(),\n",
        "                    nn.Linear(t_emb_dim, out_channels)\n",
        "                )\n",
        "                for _ in range(num_layers + 1)\n",
        "            ])\n",
        "        self.resnet_conv_second = nn.ModuleList(\n",
        "            [\n",
        "                nn.Sequential(\n",
        "                    nn.GroupNorm(norm_channels, out_channels),\n",
        "                    nn.SiLU(),\n",
        "                    nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "                )\n",
        "                for _ in range(num_layers + 1)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.attention_norms = nn.ModuleList(\n",
        "            [nn.GroupNorm(norm_channels, out_channels)\n",
        "             for _ in range(num_layers)]\n",
        "        )\n",
        "\n",
        "        self.attentions = nn.ModuleList(\n",
        "            [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
        "             for _ in range(num_layers)]\n",
        "        )\n",
        "        if self.cross_attn:\n",
        "            assert context_dim is not None, \"Context Dimension must be passed for cross attention\"\n",
        "            self.cross_attention_norms = nn.ModuleList(\n",
        "                [nn.GroupNorm(norm_channels, out_channels)\n",
        "                 for _ in range(num_layers)]\n",
        "            )\n",
        "            self.cross_attentions = nn.ModuleList(\n",
        "                [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
        "                 for _ in range(num_layers)]\n",
        "            )\n",
        "            self.context_proj = nn.ModuleList(\n",
        "                [nn.Linear(context_dim, out_channels)\n",
        "                 for _ in range(num_layers)]\n",
        "            )\n",
        "        self.residual_input_conv = nn.ModuleList(\n",
        "            [\n",
        "                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n",
        "                for i in range(num_layers + 1)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def forward(self, x, t_emb=None, context=None):\n",
        "        out = x\n",
        "\n",
        "        # First resnet block\n",
        "        resnet_input = out\n",
        "        out = self.resnet_conv_first[0](out)\n",
        "        if self.t_emb_dim is not None:\n",
        "            out = out + self.t_emb_layers[0](t_emb)[:, :, None, None]\n",
        "        out = self.resnet_conv_second[0](out)\n",
        "        out = out + self.residual_input_conv[0](resnet_input)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            # Attention Block\n",
        "            batch_size, channels, h, w = out.shape\n",
        "            in_attn = out.reshape(batch_size, channels, h * w)\n",
        "            in_attn = self.attention_norms[i](in_attn)\n",
        "            in_attn = in_attn.transpose(1, 2)\n",
        "            out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
        "            out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
        "            out = out + out_attn\n",
        "\n",
        "            if self.cross_attn:\n",
        "                assert context is not None, \"context cannot be None if cross attention layers are used\"\n",
        "                batch_size, channels, h, w = out.shape\n",
        "                in_attn = out.reshape(batch_size, channels, h * w)\n",
        "                in_attn = self.cross_attention_norms[i](in_attn)\n",
        "                in_attn = in_attn.transpose(1, 2)\n",
        "                assert context.shape[0] == x.shape[0] and context.shape[-1] == self.context_dim\n",
        "                context_proj = self.context_proj[i](context)\n",
        "                out_attn, _ = self.cross_attentions[i](in_attn, context_proj, context_proj)\n",
        "                out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
        "                out = out + out_attn\n",
        "\n",
        "\n",
        "            # Resnet Block\n",
        "            resnet_input = out\n",
        "            out = self.resnet_conv_first[i + 1](out)\n",
        "            if self.t_emb_dim is not None:\n",
        "                out = out + self.t_emb_layers[i + 1](t_emb)[:, :, None, None]\n",
        "            out = self.resnet_conv_second[i + 1](out)\n",
        "            out = out + self.residual_input_conv[i + 1](resnet_input)\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "Z6cK_F2FABzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UpBlock(nn.Module):\n",
        "    r\"\"\"\n",
        "    Up conv block with attention.\n",
        "    Sequence of following blocks\n",
        "    1. Upsample\n",
        "    1. Concatenate Down block output\n",
        "    2. Resnet block with time embedding\n",
        "    3. Attention Block\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, t_emb_dim,\n",
        "                 up_sample, num_heads, num_layers, attn, norm_channels):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.up_sample = up_sample\n",
        "        self.t_emb_dim = t_emb_dim\n",
        "        self.attn = attn\n",
        "        self.resnet_conv_first = nn.ModuleList(\n",
        "            [\n",
        "                nn.Sequential(\n",
        "                    nn.GroupNorm(norm_channels, in_channels if i == 0 else out_channels),\n",
        "                    nn.SiLU(),\n",
        "                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, stride=1,\n",
        "                              padding=1),\n",
        "                )\n",
        "                for i in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        if self.t_emb_dim is not None:\n",
        "            self.t_emb_layers = nn.ModuleList([\n",
        "                nn.Sequential(\n",
        "                    nn.SiLU(),\n",
        "                    nn.Linear(t_emb_dim, out_channels)\n",
        "                )\n",
        "                for _ in range(num_layers)\n",
        "            ])\n",
        "\n",
        "        self.resnet_conv_second = nn.ModuleList(\n",
        "            [\n",
        "                nn.Sequential(\n",
        "                    nn.GroupNorm(norm_channels, out_channels),\n",
        "                    nn.SiLU(),\n",
        "                    nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "                )\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "        if self.attn:\n",
        "            self.attention_norms = nn.ModuleList(\n",
        "                [\n",
        "                    nn.GroupNorm(norm_channels, out_channels)\n",
        "                    for _ in range(num_layers)\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            self.attentions = nn.ModuleList(\n",
        "                [\n",
        "                    nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
        "                    for _ in range(num_layers)\n",
        "                ]\n",
        "            )\n",
        "\n",
        "        self.residual_input_conv = nn.ModuleList(\n",
        "            [\n",
        "                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n",
        "                for i in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.up_sample_conv = nn.ConvTranspose2d(in_channels, in_channels,\n",
        "                                                 4, 2, 1) \\\n",
        "            if self.up_sample else nn.Identity()\n",
        "\n",
        "    def forward(self, x, out_down=None, t_emb=None):\n",
        "        # Upsample\n",
        "        x = self.up_sample_conv(x)\n",
        "\n",
        "        # Concat with Downblock output\n",
        "        if out_down is not None:\n",
        "            x = torch.cat([x, out_down], dim=1)\n",
        "\n",
        "        out = x\n",
        "        for i in range(self.num_layers):\n",
        "            # Resnet Block\n",
        "            resnet_input = out\n",
        "            out = self.resnet_conv_first[i](out)\n",
        "            if self.t_emb_dim is not None:\n",
        "                out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n",
        "            out = self.resnet_conv_second[i](out)\n",
        "            out = out + self.residual_input_conv[i](resnet_input)\n",
        "\n",
        "            # Self Attention\n",
        "            if self.attn:\n",
        "                batch_size, channels, h, w = out.shape\n",
        "                in_attn = out.reshape(batch_size, channels, h * w)\n",
        "                in_attn = self.attention_norms[i](in_attn)\n",
        "                in_attn = in_attn.transpose(1, 2)\n",
        "                out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
        "                out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
        "                out = out + out_attn\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "ljDBqTTHAErx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UpBlockUnet(nn.Module):\n",
        "    r\"\"\"\n",
        "    Up conv block with attention.\n",
        "    Sequence of following blocks\n",
        "    1. Upsample\n",
        "    1. Concatenate Down block output\n",
        "    2. Resnet block with time embedding\n",
        "    3. Attention Block\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, t_emb_dim, up_sample,\n",
        "                 num_heads, num_layers, norm_channels, cross_attn=False, context_dim=None):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.up_sample = up_sample\n",
        "        self.t_emb_dim = t_emb_dim\n",
        "        self.cross_attn = cross_attn\n",
        "        self.context_dim = context_dim\n",
        "        self.resnet_conv_first = nn.ModuleList(\n",
        "            [\n",
        "                nn.Sequential(\n",
        "                    nn.GroupNorm(norm_channels, in_channels if i == 0 else out_channels),\n",
        "                    nn.SiLU(),\n",
        "                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, stride=1,\n",
        "                              padding=1),\n",
        "                )\n",
        "                for i in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        if self.t_emb_dim is not None:\n",
        "            self.t_emb_layers = nn.ModuleList([\n",
        "                nn.Sequential(\n",
        "                    nn.SiLU(),\n",
        "                    nn.Linear(t_emb_dim, out_channels)\n",
        "                )\n",
        "                for _ in range(num_layers)\n",
        "            ])\n",
        "\n",
        "        self.resnet_conv_second = nn.ModuleList(\n",
        "            [\n",
        "                nn.Sequential(\n",
        "                    nn.GroupNorm(norm_channels, out_channels),\n",
        "                    nn.SiLU(),\n",
        "                    nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "                )\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.attention_norms = nn.ModuleList(\n",
        "            [\n",
        "                nn.GroupNorm(norm_channels, out_channels)\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.attentions = nn.ModuleList(\n",
        "            [\n",
        "                nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        if self.cross_attn:\n",
        "            assert context_dim is not None, \"Context Dimension must be passed for cross attention\"\n",
        "            self.cross_attention_norms = nn.ModuleList(\n",
        "                [nn.GroupNorm(norm_channels, out_channels)\n",
        "                 for _ in range(num_layers)]\n",
        "            )\n",
        "            self.cross_attentions = nn.ModuleList(\n",
        "                [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
        "                 for _ in range(num_layers)]\n",
        "            )\n",
        "            self.context_proj = nn.ModuleList(\n",
        "                [nn.Linear(context_dim, out_channels)\n",
        "                 for _ in range(num_layers)]\n",
        "            )\n",
        "        self.residual_input_conv = nn.ModuleList(\n",
        "            [\n",
        "                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n",
        "                for i in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.up_sample_conv = nn.ConvTranspose2d(in_channels // 2, in_channels // 2,\n",
        "                                                 4, 2, 1) \\\n",
        "            if self.up_sample else nn.Identity()\n",
        "\n",
        "    def forward(self, x, out_down=None, t_emb=None, context=None):\n",
        "        x = self.up_sample_conv(x)\n",
        "        if out_down is not None:\n",
        "            x = torch.cat([x, out_down], dim=1)\n",
        "\n",
        "        out = x\n",
        "        for i in range(self.num_layers):\n",
        "            # Resnet\n",
        "            resnet_input = out\n",
        "            out = self.resnet_conv_first[i](out)\n",
        "            if self.t_emb_dim is not None:\n",
        "                out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n",
        "            out = self.resnet_conv_second[i](out)\n",
        "            out = out + self.residual_input_conv[i](resnet_input)\n",
        "            # Self Attention\n",
        "            batch_size, channels, h, w = out.shape\n",
        "            in_attn = out.reshape(batch_size, channels, h * w)\n",
        "            in_attn = self.attention_norms[i](in_attn)\n",
        "            in_attn = in_attn.transpose(1, 2)\n",
        "            out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
        "            out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
        "            out = out + out_attn\n",
        "            # Cross Attention\n",
        "            if self.cross_attn:\n",
        "                assert context is not None, \"context cannot be None if cross attention layers are used\"\n",
        "                batch_size, channels, h, w = out.shape\n",
        "                in_attn = out.reshape(batch_size, channels, h * w)\n",
        "                in_attn = self.cross_attention_norms[i](in_attn)\n",
        "                in_attn = in_attn.transpose(1, 2)\n",
        "                assert len(context.shape) == 3, \\\n",
        "                    \"Context shape does not match B,_,CONTEXT_DIM\"\n",
        "                assert context.shape[0] == x.shape[0] and context.shape[-1] == self.context_dim,\\\n",
        "                    \"Context shape does not match B,_,CONTEXT_DIM\"\n",
        "                context_proj = self.context_proj[i](context)\n",
        "                out_attn, _ = self.cross_attentions[i](in_attn, context_proj, context_proj)\n",
        "                out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
        "                out = out + out_attn\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "WaOlfl6uAIUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgcTwlcuJgV3"
      },
      "outputs": [],
      "source": [
        "class VQVAE(nn.Module):\n",
        "  def __init__(self, in_channels, im_channels): # Added missing arguments\n",
        "    super().__init__()\n",
        "    self.down_channels=[64,128,256,256]\n",
        "    self.mid_channels=[256,256]\n",
        "    self.down_sample=[True,True,True]\n",
        "    self.num_down_layers=2\n",
        "    self.num_mid_layers=2\n",
        "    self.num_up_layers=2\n",
        "    self.norm_channels=32\n",
        "    self.attn=[False,False,False]\n",
        "    self.num_heads=4\n",
        "    self.z_channels=3\n",
        "    self.codebook_size=8192\n",
        "    self.up_sample=list(reversed(self.down_sample))\n",
        "    self.encoder_conv_in=nn.Conv2d(in_channels,self.down_channels[0],kernel_size=3,padding=(1,1))\n",
        "\n",
        "    self.encoder_downs = nn.ModuleList([])\n",
        "    for i in range(len(self.down_channels) - 1):\n",
        "      self.encoder_downs.append(DownBlock(self.down_channels[i], self.down_channels[i+1], t_emb_dim=None, down_sample=self.down_sample[i], num_heads=self.num_heads, num_layers=self.num_down_layers, attn=self.attn[i], norm_channels=self.norm_channels))\n",
        "    # Fixed indentation for the following lines\n",
        "    self.encoder_mids = nn.ModuleList([])\n",
        "    for i in range(len(self.mid_channels) - 1):\n",
        "            self.encoder_mids.append(MidBlock(self.mid_channels[i], self.mid_channels[i + 1],\n",
        "                                              t_emb_dim=None,\n",
        "                                              num_heads=self.num_heads,\n",
        "                                              num_layers=self.num_mid_layers,\n",
        "                                              norm_channels=self.norm_channels))\n",
        "\n",
        "    self.encoder_norm_out = nn.GroupNorm(self.norm_channels, self.down_channels[-1])\n",
        "    self.encoder_conv_out = nn.Conv2d(self.down_channels[-1], self.z_channels, kernel_size=3, padding=1)\n",
        "\n",
        "    # Pre Quantization Convolution\n",
        "    self.pre_quant_conv = nn.Conv2d(self.z_channels, self.z_channels, kernel_size=1)\n",
        "\n",
        "    # Codebook\n",
        "    self.embedding = nn.Embedding(self.codebook_size, self.z_channels)\n",
        "    ####################################################\n",
        "\n",
        "    ##################### Decoder ######################\n",
        "\n",
        "    # Post Quantization Convolution\n",
        "    self.post_quant_conv = nn.Conv2d(self.z_channels, self.z_channels, kernel_size=1)\n",
        "    self.decoder_conv_in = nn.Conv2d(self.z_channels, self.mid_channels[-1], kernel_size=3, padding=(1, 1))\n",
        "\n",
        "    # Midblock + Upblock\n",
        "    self.decoder_mids = nn.ModuleList([])\n",
        "    for i in reversed(range(1, len(self.mid_channels))):\n",
        "        self.decoder_mids.append(MidBlock(self.mid_channels[i], self.mid_channels[i - 1],\n",
        "                                          t_emb_dim=None,\n",
        "                                          num_heads=self.num_heads,\n",
        "                                          num_layers=self.num_mid_layers,\n",
        "                                          norm_channels=self.norm_channels))\n",
        "\n",
        "    self.decoder_layers = nn.ModuleList([])\n",
        "    for i in reversed(range(1, len(self.down_channels))):\n",
        "        self.decoder_layers.append(UpBlock(self.down_channels[i], self.down_channels[i - 1],\n",
        "                                           t_emb_dim=None,))\n",
        "\n",
        "    def quantize(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "\n",
        "        # B, C, H, W -> B, H, W, C\n",
        "        x = x.permute(0, 2, 3, 1)\n",
        "\n",
        "        # B, H, W, C -> B, H*W, C\n",
        "        x = x.reshape(x.size(0), -1, x.size(-1))\n",
        "\n",
        "        # Find nearest embedding/codebook vector\n",
        "        # dist between (B, H*W, C) and (B, K, C) -> (B, H*W, K)\n",
        "        dist = torch.cdist(x, self.embedding.weight[None, :].repeat((x.size(0), 1, 1)))\n",
        "        # (B, H*W)\n",
        "        min_encoding_indices = torch.argmin(dist, dim=-1)\n",
        "\n",
        "        # Replace encoder output with nearest codebook\n",
        "        # quant_out -> B*H*W, C\n",
        "        quant_out = torch.index_select(self.embedding.weight, 0, min_encoding_indices.view(-1))\n",
        "\n",
        "        # x -> B*H*W, C\n",
        "        x = x.reshape((-1, x.size(-1)))\n",
        "        commmitment_loss = torch.mean((quant_out.detach() - x) ** 2)\n",
        "        codebook_loss = torch.mean((quant_out - x.detach()) ** 2)\n",
        "        quantize_losses = {\n",
        "            'codebook_loss': codebook_loss,\n",
        "            'commitment_loss': commmitment_loss\n",
        "        }\n",
        "        # Straight through estimation\n",
        "        quant_out = x + (quant_out - x).detach()\n",
        "\n",
        "        # quant_out -> B, C, H, W\n",
        "        quant_out = quant_out.reshape((B, H, W, C)).permute(0, 3, 1, 2)\n",
        "        min_encoding_indices = min_encoding_indices.reshape((-1, quant_out.size(-2), quant_out.size(-1)))\n",
        "        return quant_out, quantize_losses, min_encoding_indices\n",
        "\n",
        "    def encode(self, x):\n",
        "        out = self.encoder_conv_in(x)\n",
        "        for idx, down in enumerate(self.encoder_layers):\n",
        "            out = down(out)\n",
        "        for mid in self.encoder_mids:\n",
        "            out = mid(out)\n",
        "        out = self.encoder_norm_out(out)\n",
        "        out = nn.SiLU()(out)\n",
        "        out = self.encoder_conv_out(out)\n",
        "        out = self.pre_quant_conv(out)\n",
        "        out, quant_losses, _ = self.quantize(out)\n",
        "        return out, quant_losses\n",
        "\n",
        "    def decode(self, z):\n",
        "        out = z\n",
        "        out = self.post_quant_conv(out)\n",
        "        out = self.decoder_conv_in(out)\n",
        "        for mid in self.decoder_mids:\n",
        "            out = mid(out)\n",
        "        for idx, up in enumerate(self.decoder_layers):\n",
        "            out = up(out)\n",
        "\n",
        "        out = self.decoder_norm_out(out)\n",
        "        out = nn.SiLU()(out)\n",
        "        out = self.decoder_conv_out(out)\n",
        "        return out\n",
        "\n",
        "    def forward(self, x):\n",
        "        z, quant_losses = self.encode(x)\n",
        "        out = self.decode(z)\n",
        "        return out, z, quant_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWr74i4AQ6HM"
      },
      "source": [
        "##Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUD8weqRQwY7"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, im_channels=3,\n",
        "                 conv_channels=[64, 128, 256],\n",
        "                 kernels=[4,4,4,4],\n",
        "                 strides=[2,2,2,1],\n",
        "                 paddings=[1,1,1,1]):\n",
        "        super().__init__()\n",
        "        self.im_channels = im_channels\n",
        "        activation = nn.LeakyReLU(0.2)\n",
        "        layers_dim = [self.im_channels] + conv_channels + [1]\n",
        "        self.layers = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Conv2d(layers_dim[i], layers_dim[i + 1],\n",
        "                          kernel_size=kernels[i],\n",
        "                          stride=strides[i],\n",
        "                          padding=paddings[i],\n",
        "                          bias=False if i !=0 else True),\n",
        "                nn.BatchNorm2d(layers_dim[i + 1]) if i != len(layers_dim) - 2 and i != 0 else nn.Identity(),\n",
        "                activation if i != len(layers_dim) - 2 else nn.Identity()\n",
        "            )\n",
        "            for i in range(len(layers_dim) - 1)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x\n",
        "        for layer in self.layers:\n",
        "            out = layer(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKKhbisPRZfS"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from diffusion_utils import load_latents\n",
        "from celeb_dataset import CelebDataset\n",
        "from mnist_dataset import MnistDataset\n",
        "import torch\n",
        "from torch.optim import Adam\n",
        "import os\n",
        "import torchvision\n",
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "import yaml\n",
        "import argparse\n",
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ib9hjMItQsL3"
      },
      "outputs": [],
      "source": [
        "def train(args):\n",
        "      model=VQVAE(in_channels=64,im_channels=3).to(device)\n",
        "      lpips_model=LPIPS().eval().to(device)\n",
        "      discriminator=Discriminator(im_channels=3).to(device)\n",
        "\n",
        "      im_dataset=CelebDataset(split='train',\n",
        "                              im_path='data/celebhq',\n",
        "                              im_size=256,\n",
        "                              im_channels=3)\n",
        "      data_loader=DataLoader(im_dataset,batch_size=16,shuffle=True)\n",
        "      num_epochs=20\n",
        "      recon_criterion=torch.nn.MSELoss()\n",
        "      disc_criterion=torch.nn.BCEWithLogitsLoss()\n",
        "      optimizer_d=Adam(discriminator.parameters(),lr=1e-5,betas=(0.5,0.999))\n",
        "      optimizer_g=Adam(model.parameters(),lr=1e-5,betas=(0.5,0.999))\n",
        "      disc_step_start=15000\n",
        "      step_count=0\n",
        "\n",
        "      for epoch_idx in range(num_epochs):\n",
        "        for im in tqdm(data_loader):\n",
        "          optimizer_g.zero_grad()\n",
        "          optimizer_d.zero_grad()\n",
        "          step_count+=1\n",
        "          im=im.float().to(device)\n",
        "\n",
        "          model_output=model(im)\n",
        "          output,z,quantize_losses=model_output\n",
        "\n",
        "          recon_loss=recon_criterion(output,im)\n",
        "          g_loss=(recon_loss+(1*quantize_losses['codebook_loss'])+(1*quantize_losses['commitment_loss']))\n",
        "          if step_count>disc_step_start:\n",
        "            disc_fake_pred=discriminator(output)\n",
        "            disc_fake_loss=disc_criterion(disc_fake_pred,torch.ones(device=disc_fake_pred.device))\n",
        "            g_loss+=0.5*disc_fake_loss\n",
        "\n",
        "          lpips_loss=torch.nn(lpips_model(output,im).mean())\n",
        "          g_loss+=lpips_loss\n",
        "          g_loss.backward()\n",
        "          optimizer_g.step()\n",
        "\n",
        "\n",
        "          if step_count>disc_step_start:\n",
        "            fake=output\n",
        "            disc_fack_pred=discriminator(fake.detach())\n",
        "            disc_real_pred=discriminator(im)\n",
        "            disc_real_loss=disc_criterion(disc_real_pred,torch.zeros(disc_fake_pred.shape,device=disc_real_pred.device))\n",
        "            disc_loss=0.5*(disc_fake_loss+disc_real_loss)/2\n",
        "            disc_loss.backward()\n",
        "            optimizer_d.step()\n",
        "\n",
        "        torch.save(model.state_dict(),'vqvae_autoencoder_ckpt.pth')\n",
        "        torch.save(discriminator.state_dict(),'vqvae_discriminator_ckpt.pth')\n",
        "\n",
        "      print('Done Training..')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser(description='Arguments for ddpm training')\n",
        "    parser.add_argument('--config', dest='config_path',\n",
        "                        default='config/mnist.yaml', type=str)\n",
        "    args = parser.parse_args()\n",
        "    train(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybdNnkPhXYTQ"
      },
      "outputs": [],
      "source": [
        "def get_time_embedding(time_steps, temb_dim):\n",
        "    r\"\"\"\n",
        "    Convert time steps tensor into an embedding using the\n",
        "    sinusoidal time embedding formula\n",
        "    :param time_steps: 1D tensor of length batch size\n",
        "    :param temb_dim: Dimension of the embedding\n",
        "    :return: BxD embedding representation of B time steps\n",
        "    \"\"\"\n",
        "    assert temb_dim % 2 == 0, \"time embedding dimension must be divisible by 2\"\n",
        "\n",
        "    # factor = 10000^(2i/d_model)\n",
        "    factor = 10000 ** ((torch.arange(\n",
        "        start=0, end=temb_dim // 2, dtype=torch.float32, device=time_steps.device) / (temb_dim // 2))\n",
        "    )\n",
        "\n",
        "    # pos / factor\n",
        "    # timesteps B -> B, 1 -> B, temb_dim\n",
        "    t_emb = time_steps[:, None].repeat(1, temb_dim // 2) / factor\n",
        "    t_emb = torch.cat([torch.sin(t_emb), torch.cos(t_emb)], dim=-1)\n",
        "    return t_emb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBufsPHUV8UK"
      },
      "outputs": [],
      "source": [
        "class Unet(nn.Module):\n",
        "    r\"\"\"\n",
        "    Unet model comprising\n",
        "    Down blocks, Midblocks and Uplocks\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, im_channels=3):\n",
        "        super().__init__()\n",
        "        self.down_channels = [256,384,512,768]\n",
        "        self.mid_channels = [768,512]\n",
        "        self.t_emb_dim = 512\n",
        "        self.down_sample = [True,True,True]\n",
        "        self.num_down_layers = 2\n",
        "        self.num_mid_layers = 2\n",
        "        self.num_up_layers = 2\n",
        "        self.attns = [True,True,True]\n",
        "        self.norm_channels = 32\n",
        "        self.num_heads = 16\n",
        "        self.conv_out_channels = 128\n",
        "\n",
        "        assert self.mid_channels[0] == self.down_channels[-1]\n",
        "        assert self.mid_channels[-1] == self.down_channels[-2]\n",
        "        assert len(self.down_sample) == len(self.down_channels) - 1\n",
        "        assert len(self.attns) == len(self.down_channels) - 1\n",
        "\n",
        "        # Initial projection from sinusoidal time embedding\n",
        "        self.t_proj = nn.Sequential(\n",
        "            nn.Linear(self.t_emb_dim, self.t_emb_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(self.t_emb_dim, self.t_emb_dim)\n",
        "        )\n",
        "\n",
        "        self.up_sample = list(reversed(self.down_sample))\n",
        "        self.conv_in = nn.Conv2d(im_channels, self.down_channels[0], kernel_size=3, padding=1)\n",
        "\n",
        "        self.downs = nn.ModuleList([])\n",
        "        for i in range(len(self.down_channels) - 1):\n",
        "            self.downs.append(DownBlock(self.down_channels[i], self.down_channels[i + 1], self.t_emb_dim,\n",
        "                                        down_sample=self.down_sample[i],\n",
        "                                        num_heads=self.num_heads,\n",
        "                                        num_layers=self.num_down_layers,\n",
        "                                        attn=self.attns[i], norm_channels=self.norm_channels))\n",
        "\n",
        "        self.mids = nn.ModuleList([])\n",
        "        for i in range(len(self.mid_channels) - 1):\n",
        "            self.mids.append(MidBlock(self.mid_channels[i], self.mid_channels[i + 1], self.t_emb_dim,\n",
        "                                      num_heads=self.num_heads,\n",
        "                                      num_layers=self.num_mid_layers,\n",
        "                                      norm_channels=self.norm_channels))\n",
        "\n",
        "        self.ups = nn.ModuleList([])\n",
        "        for i in reversed(range(len(self.down_channels) - 1)):\n",
        "            self.ups.append(UpBlock(self.down_channels[i] * 2, self.down_channels[i - 1] if i != 0 else self.conv_out_channels,\n",
        "                                    self.t_emb_dim, up_sample=self.down_sample[i],\n",
        "                                        num_heads=self.num_heads,\n",
        "                                        num_layers=self.num_up_layers,\n",
        "                                        norm_channels=self.norm_channels))\n",
        "\n",
        "        self.norm_out = nn.GroupNorm(self.norm_channels, self.conv_out_channels)\n",
        "        self.conv_out = nn.Conv2d(self.conv_out_channels, im_channels, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        # Shapes assuming downblocks are [C1, C2, C3, C4]\n",
        "        # Shapes assuming midblocks are [C4, C4, C3]\n",
        "        # Shapes assuming downsamples are [True, True, False]\n",
        "        # B x C x H x W\n",
        "        out = self.conv_in(x)\n",
        "        # B x C1 x H x W\n",
        "\n",
        "        # t_emb -> B x t_emb_dim\n",
        "        t_emb = get_time_embedding(torch.as_tensor(t).long(), self.t_emb_dim)\n",
        "        t_emb = self.t_proj(t_emb)\n",
        "\n",
        "        down_outs = []\n",
        "\n",
        "        for idx, down in enumerate(self.downs):\n",
        "            down_outs.append(out)\n",
        "            out = down(out, t_emb)\n",
        "        # down_outs  [B x C1 x H x W, B x C2 x H/2 x W/2, B x C3 x H/4 x W/4]\n",
        "        # out B x C4 x H/4 x W/4\n",
        "\n",
        "        for mid in self.mids:\n",
        "            out = mid(out, t_emb)\n",
        "        # out B x C3 x H/4 x W/4\n",
        "\n",
        "        for up in self.ups:\n",
        "            down_out = down_outs.pop()\n",
        "            out = up(out, down_out, t_emb)\n",
        "            # out [B x C2 x H/4 x W/4, B x C1 x H/2 x W/2, B x 16 x H x W]\n",
        "        out = self.norm_out(out)\n",
        "        out = nn.SiLU()(out)\n",
        "        out = self.conv_out(out)\n",
        "        # out B x C x H x W\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WRDytNjX86v"
      },
      "outputs": [],
      "source": [
        "class LinearNoiseScheduler:\n",
        "    r\"\"\"\n",
        "    Class for the linear noise scheduler that is used in DDPM.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_timesteps, beta_start, beta_end):\n",
        "        self.num_timesteps = num_timesteps\n",
        "        self.beta_start = beta_start\n",
        "        self.beta_end = beta_end\n",
        "        # Mimicking how compvis repo creates schedule\n",
        "        self.betas = (\n",
        "                torch.linspace(beta_start ** 0.5, beta_end ** 0.5, num_timesteps) ** 2\n",
        "        )\n",
        "        self.alphas = 1. - self.betas\n",
        "        self.alpha_cum_prod = torch.cumprod(self.alphas, dim=0)\n",
        "        self.sqrt_alpha_cum_prod = torch.sqrt(self.alpha_cum_prod)\n",
        "        self.sqrt_one_minus_alpha_cum_prod = torch.sqrt(1 - self.alpha_cum_prod)\n",
        "\n",
        "    def add_noise(self, original, noise, t):\n",
        "        r\"\"\"\n",
        "        Forward method for diffusion\n",
        "        :param original: Image on which noise is to be applied\n",
        "        :param noise: Random Noise Tensor (from normal dist)\n",
        "        :param t: timestep of the forward process of shape -> (B,)\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        original_shape = original.shape\n",
        "        batch_size = original_shape[0]\n",
        "\n",
        "        sqrt_alpha_cum_prod = self.sqrt_alpha_cum_prod.to(original.device)[t].reshape(batch_size)\n",
        "        sqrt_one_minus_alpha_cum_prod = self.sqrt_one_minus_alpha_cum_prod.to(original.device)[t].reshape(batch_size)\n",
        "\n",
        "        # Reshape till (B,) becomes (B,1,1,1) if image is (B,C,H,W)\n",
        "        for _ in range(len(original_shape) - 1):\n",
        "            sqrt_alpha_cum_prod = sqrt_alpha_cum_prod.unsqueeze(-1)\n",
        "        for _ in range(len(original_shape) - 1):\n",
        "            sqrt_one_minus_alpha_cum_prod = sqrt_one_minus_alpha_cum_prod.unsqueeze(-1)\n",
        "\n",
        "        # Apply and Return Forward process equation\n",
        "        return (sqrt_alpha_cum_prod.to(original.device) * original\n",
        "                + sqrt_one_minus_alpha_cum_prod.to(original.device) * noise)\n",
        "\n",
        "    def sample_prev_timestep(self, xt, noise_pred, t):\n",
        "        r\"\"\"\n",
        "            Use the noise prediction by model to get\n",
        "            xt-1 using xt and the nosie predicted\n",
        "        :param xt: current timestep sample\n",
        "        :param noise_pred: model noise prediction\n",
        "        :param t: current timestep we are at\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        x0 = ((xt - (self.sqrt_one_minus_alpha_cum_prod.to(xt.device)[t] * noise_pred)) /\n",
        "              torch.sqrt(self.alpha_cum_prod.to(xt.device)[t]))\n",
        "        x0 = torch.clamp(x0, -1., 1.)\n",
        "\n",
        "        mean = xt - ((self.betas.to(xt.device)[t]) * noise_pred) / (self.sqrt_one_minus_alpha_cum_prod.to(xt.device)[t])\n",
        "        mean = mean / torch.sqrt(self.alphas.to(xt.device)[t])\n",
        "\n",
        "        if t == 0:\n",
        "            return mean, x0\n",
        "        else:\n",
        "            variance = (1 - self.alpha_cum_prod.to(xt.device)[t - 1]) / (1.0 - self.alpha_cum_prod.to(xt.device)[t])\n",
        "            variance = variance * self.betas.to(xt.device)[t]\n",
        "            sigma = variance ** 0.5\n",
        "            z = torch.randn(xt.shape).to(xt.device)\n",
        "\n",
        "            # OR\n",
        "            # variance = self.betas[t]\n",
        "            # sigma = variance ** 0.5\n",
        "            # z = torch.randn(xt.shape).to(xt.device)\n",
        "            return mean + sigma * z, x0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6o-U2xobqOK"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "def train(args):\n",
        "      scheduler=LinearNoiseScheduler(num_timesteps=1000,beta_start=1e-4,beta_end=0.02)\n",
        "      im_dataset=CelebDataset(split='train',\n",
        "                              im_path='/content/drive/MyDrive/SOC 2024/data/CelebA-HQ-img',\n",
        "                              im_size=256,\n",
        "                              im_channels=3)\n",
        "      print(\"Length of dataset:\", len(im_dataset))\n",
        "      data_loader=DataLoader(im_dataset,batch_size=64,shuffle=True)\n",
        "      z_channels=3\n",
        "      model=Unet(im_channels=z_channels).to(device)\n",
        "      model.train()\n",
        "\n",
        "      vae=VQVAE(in_channels=64,im_channels=3).to(device)\n",
        "      vae.load_state_dict(torch.load('vqvae_autoencoder_ckt.pth',map_location=device))\n",
        "      vae.eval()\n",
        "      for param in vae.parameters():\n",
        "        param.requires_grad=False\n",
        "\n",
        "      num_epochs=50\n",
        "      optimizer=Adam(model.parameters(),lr=5e-5)\n",
        "      criterion=torch.nn.MSELoss()\n",
        "\n",
        "\n",
        "      for epoch_idx in range(num_epochs):\n",
        "        losses=[]\n",
        "        for im in tqdm(data_loader):\n",
        "          im=im.float().to(device)\n",
        "          optimizer.zero_grad()\n",
        "          with torch.no_grad():\n",
        "            im,_=vae.encode(im)\n",
        "          noise=torch.randn_like(im)\n",
        "          noise=noise.to(device)\n",
        "          t=torch.randint(0,1000,(im.shape[0],)).to(device)\n",
        "          noise_im=scheduler.add_noise(im,noise,t)\n",
        "          noise_pred=model(noise_im,t)\n",
        "          loss=criterion(noise_pred, noise)\n",
        "          losses.append(loss.item())\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "        print('Finished epoch:{}  | Loss :{:4f}'.format(\n",
        "              *(epoch_idx + 1), np.mean(losses)\n",
        "          ))\n",
        "      torch.save(model.state_dict(), 'ddpm_vqvae_ckpt.pth')\n",
        "\n",
        "      print(\"Training done\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9NGOh2VxdkH"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Create a Namespace object directly with your desired arguments\n",
        "    args = argparse.Namespace(\n",
        "        batch_size=64,\n",
        "        im_size=256,\n",
        "        im_channels=3,\n",
        "        num_epochs=50,\n",
        "        log_interval=50,\n",
        "        checkpoint_name=\"ddpm_vqvae_ckpt.pth\"\n",
        "    )\n",
        "    train(args)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}